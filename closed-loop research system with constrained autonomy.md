# â€œfully-fledged autonomous research systemâ€ idea

What I want:

> build â†’ choose symbols â†’ choose intervals â†’ backtest â†’ review â†’ expand â†’ optimize â†’ iterate

This **is doable**, but with **constrain agency**.

---

### Idea: â€œThe system should choose symbols & intervals by itselfâ€

Partially correct â€” but not blindly.

#### Correct approach

The system should choose **candidates**, not roam freely.

That choice must be:

* Spec-driven
* Hypothesis-driven
* Budgeted

Example:

Spec metadata:

```json
{
  "style": "mean_reversion",
  "holding_period": "short",
  "volatility_preference": "high",
  "market_type": "equities"
}
```

From this, rules (code, not LLM):

* Mean reversion â†’ lower TFs first
* Short holding â†’ avoid daily
* High vol â†’ prefer mid-cap / volatile names

LLM:

* Ranks symbol candidates
* Explains why expansion makes sense

Code:

* Executes
* Enforces budget
* Stops explosion

---

### Idea: â€œSystem should detect sector/industry edgeâ€

This is reasonable â€” but subtle.

Do **not** let the LLM discover this from raw PnL.

Instead:

* You compute performance by sector
* You compute stability across symbols
* You pass *aggregates* to LLM

LLM answers:

* â€œIs this concentration meaningful or noise?â€
* â€œShould we expand or constrain universe?â€

---

## Key correction

> â€œMore autonomy = better researchâ€
Thatâ€™s false.

**Better constraints = better research**.

Autonomy without:

* gates
* budgets
* invariants

just accelerates overfitting.

The design is *good* â€” but only if:

* LLMs reason
* Code enforces
* Specs constrain


# 1. The architecture / workflow

What you will be building is a **closed-loop research system with constrained autonomy**.

### High-level loop (single sentence)

> **LLM proposes intent â†’ deterministic system executes â†’ deterministic gates judge â†’ LLM reasons about next move â†’ system enforces limits**

### Expanded workflow (step-by-step)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User Strategy Idea      â”‚
â”‚    (text / pseudo / Pine)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Translator Agent (LLM)  â”‚
â”‚    - Extract intent        â”‚
â”‚    - Normalize logic       â”‚
â”‚    - Classify strategy     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Strategy Spec (JSON)    â”‚
â”‚    - Declarative           â”‚
â”‚    - Versioned             â”‚
â”‚    - Immutable intent      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Compiler (deterministic)â”‚
â”‚    - Spec â†’ Base subclass  â”‚
â”‚    - Indicator wiring      â”‚
â”‚    - Param validation      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Research Director       â”‚
â”‚    (hybrid: code + LLM)    â”‚
â”‚    - Select symbols        â”‚
â”‚    - Select intervals      â”‚
â”‚    - Enforce budgets       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Execution Engine        â”‚
â”‚    (your current system)  â”‚
â”‚    - Download data (if req)â”‚
â”‚    - Run backtest          â”‚
â”‚    - Produce metrics       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Reviewer (deterministic)â”‚
â”‚    - Hard gates            â”‚
â”‚    - Reject garbage early  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. Reviewer (LLM layer)    â”‚
â”‚    - Diagnose behavior     â”‚
â”‚    - Match intent vs resultâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. Director Decision       â”‚
â”‚    - Expand universe?      â”‚
â”‚    - Change timeframe?     â”‚
â”‚    - Optimize params?      â”‚
â”‚    - Kill strategy?        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
        (loop or terminate)
```

**Key rule:**
ğŸ‘‰ *Only steps 2, 8, 9 involve LLMs, and 5 hybrid code-LLM*
Everything else is deterministic and auditable.

---

# 2. Constraints: gates, budgets, invariants (this is the core)

Without these, autonomy = self-delusion at scale.

## A. Gates (binary kill switches)

Gates are **non-negotiable**.
If a gate fails â†’ **strategy dies immediately**. No reasoning, no optimization.

### Example gate categories

#### 1. Statistical viability gates

```text
min_trades_per_year â‰¥ 100
max_drawdown â‰¤ 25%
avg_trade_return â‰¥ 0.2%
```

#### 2. Distribution sanity gates

* Top 5 trades contribute â‰¤ X% of total PnL
* No single day/week dominates equity curve
* Trade spacing not absurdly clustered

#### 3. Robustness gates

* Performance does not collapse (>50%) across:

  * train / test
  * first half / second half
  * random subsample

#### 4. Complexity gates

* Max indicators: e.g. 5
* Max conditions per side: e.g. 6
* Max parameters optimized: e.g. 6

If any gate fails:

```
status = DEAD
reason = <which gate failed>
```

LLM is **not consulted** here.

---

## B. Invariants (rules that must *never* be violated)

Invariants protect you from silent research corruption.

### Examples (non-exhaustive)

* Strategy logic cannot change after first review
  (only parameters may change)
* Indicators cannot be added after initial compile
* Timeframe changes must follow allowed ladder (e.g. 5m â†’ 15m â†’ 1h)
* Optimization cannot start until OOS pass
* Reviewer cannot approve strategy it didnâ€™t reject first

Think of invariants as **constitutional law**.

---

## C. Budgets (this is where most people fail)

Budgets limit **exploration**, not quality.

### Core insight

> You donâ€™t prevent overfitting by being smart.
> You prevent it by **running out of budget before nonsense explodes**.

---

# 3. Research budget system (anti-combinatorial-explosion)

This is the most important part of your question.

## A. What causes combinatorial explosion?

Letâ€™s be explicit:

* Symbols Ã—
* Timeframes Ã—
* Parameter combinations Ã—
* Regimes Ã—
* Re-runs after optimization

Without limits, this grows **exponentially**.

---

## B. Introduce a â€œresearch currencyâ€

Every action **costs credits**.

### Example budget model

```python
TOTAL_BUDGET = 1_000 credits
```

### Cost table (example)

| Action                            | Cost |
| --------------------------------- | ---- |
| Initial backtest (1 symbol, 1 TF) | 10   |
| Add new symbol                    | 5    |
| Add new timeframe                 | 8    |
| Parameter optimization run        | 20   |
| Walk-forward test                 | 25   |
| Regime expansion                  | 15   |
| Re-optimization                   | 30   |

---

## C. Director must justify spending budget

Before an action, the **Director Agent** must produce:

```json
{
  "action": "expand_symbol_universe",
  "cost": 25,
  "expected_information_gain": "validate sector robustness",
  "justification": "strategy shows stable behavior in 3/5 stocks of same sector"
}
```

Then:

* Code checks budget
* Action executes
* Budget decremented
* Decision logged

If budget exhausted â†’ strategy frozen.

---

## D. Budget reset rules (important)

You do **not** reset budget on:

* Minor metric improvement
* Optimization success
* Better Sharpe

You **may** reset or extend budget only if:

* Strategy passes all gates
* Shows stability across *new dimension* (e.g. sector)
* Director explicitly promotes it to â€œcandidate libraryâ€

This forces **promotion discipline**.

---

# 4. How autonomy actually works (corrected intuition)

Wanted:

> â€œfull-fledged autonomous researchâ€

Hereâ€™s the corrected version:

* LLMs **hypothesize**
* Code **tests**
* Gates **kill**
* Budgets **limit**
* Invariants **protect**
* LLMs **interpret**
* Director **chooses next experiment**
* System **stops itself**

Thatâ€™s autonomy **with brakes**.

---

# 5. points to note

### 1.
LLM should not explore widely to find hidden edges.
Edges emerge from **constraint-guided exploration**, not randomness.

---

### 2.
Optimization should not be aggressive once something works.
Optimization is where most strategies die *quietly*.

---

### 3.
> â€œIf LLM explains it well, itâ€™s probably realâ€
This narrative is cheap.
Stability is expensive.

---

# 6. Current system is already 70% there

Already have:

* A clean `Base` class
* Deterministic execution
* Optimization hooks
* Indicator separation

What youâ€™re adding is **governance**, not magic.

---

## Final distilled blueprint

* **Architecture**: Spec-driven, engine-isolated, loop-controlled
* **Gates**: Binary, harsh, early
* **Invariants**: Constitutional, never overridden
* **Budgets**: Credit-based, action-priced
* **LLMs**: Reason, decide, explain â€” never measure or execute
